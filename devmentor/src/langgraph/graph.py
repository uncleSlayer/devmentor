from langgraph.graph import StateGraph
from src.langgraph.state import SingleQuestionChatState
from src.ai import llm, retriver
from src.langgraph.tools import youtube_search_tool
from youtube_search import YoutubeSearch
from langchain_openai import ChatOpenAI
from config.env import settings

single_question_chat_graph = StateGraph(SingleQuestionChatState)


class SingleQuestionGraph:

    def generate_youtube_suggestions(state):
        
        """
        This function uses a tool to generate youtube suggestions based on the question.
        The tool takes the question as input and returns a list of youtube video ids.
        """

        question = state["question"]

        # Chat gpt helped me write the tool calling function. Better to use something simple in this case, I can't be wrtting this without any proper docs.
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.5, api_key=settings.OPENAI_API_KEY).bind(
            functions=[{
            "name": "youtube_search_tool",
            "description": "Useful for searching YouTube videos based on a query.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The YouTube search query"
                    }
                },
                "required": ["query"]
            }
        }]
        )

        # youtube_suggestions: str = youtube_search_tool.run(question)

        youtube_suggestions = llm.invoke(f"""
        You are a helpful youtube search assistant. You use the youtube_search_tool to search for youtube videos based on the question.
        You return a list of youtube videos.
                                              
        Question: {question}
        """)

        print("youtube_suggestions", youtube_suggestions)

        state["youtube_suggestions"] = youtube_suggestions

        return state

    def generate_answer(state):
        """
        This function is called when the user asks a question.
        It retrieves the relevant documents from the vector store and passes them to the LLM.
        The LLM generates an answer based on the question and the retrieved documents.
        The answer is then stored in the langgraph state.
        """

        question = state["question"]

        retrived_documents = retriver.invoke(question)

        answer = llm.invoke(
            f"""
            You are a helpful assistant that answers questions based on the provided context.
            If you don't know the answer, just say that you don't know. Don't try to make up an answer.
            If the user asks for code, write all the code in one code block. Don't write code that can't be run in a basic python repl.
            Your code example will be ran in a docker container of a python repl.

            Document context:
            {retrived_documents}

            Question: "{question}"
        """
        )

        state["answer"] = dict(answer).get("content")

        return state

    def generate_code_block(state):
        """
        This function is called when the user asks for code.
        It goes through the langgraph state and extracts the answer out of it.
        It then parses out the code block from the answer.
        The code block is then passed to the LLM to make it properly formatted.
        The formatted code block is then stored in the langgraph state.
        """

        answer = state["answer"]

        code_block = llm.invoke(
            f"""

            You are a helpful assistant that parses out the code block from the input.
            The input answer given to you is geneated by an ai assistant.
            Your job is to parse out the code block from the answer. You are also responsible for the overall code quality.
            You can change the code block to be more readable and better formatted and if the code can not be run in a basic python repl, 
            you change the code block to be a valid python code block.
            The code must be so that it can be run in a basic python repl.
            If you think the input doesn't have a code block, you return "None".
            Your output must not contain anything other than the code block.

            Answer generated by the ai assistant:
            {answer}
        """
        )

        state["code_block"] = code_block.content

        return state

single_question_chat_graph.add_node(
    "generate_answer", SingleQuestionGraph.generate_answer
)

single_question_chat_graph.add_node(
    "generate_code_block", SingleQuestionGraph.generate_code_block
)

single_question_chat_graph.add_node(
    "generate_youtube_suggestions", SingleQuestionGraph.generate_youtube_suggestions
)

single_question_chat_graph.add_edge("generate_youtube_suggestions", "generate_answer")

single_question_chat_graph.add_edge("generate_answer", "generate_code_block")

single_question_chat_graph.set_entry_point("generate_youtube_suggestions")

runner = single_question_chat_graph.compile({})


def generate_answer(question: str):

    return runner.invoke({"question": question, "answer": "", "code_block": "", "youtube_suggestions": []})
